{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Ggenerative Adversarial Networks (GANs) using Keras**"
      ],
      "metadata": {
        "id": "kz4iNEvz4pYe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLJrHd994niq"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow-cpu==2.16.2\n",
        "!pip install matplotlib\n",
        "# Suppress warnings and set environment variables\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and preprocess the MNIST dataset for training a GAN."
      ],
      "metadata": {
        "id": "p5d0f5rs5MiU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import warnings\n",
        "\n",
        "# Suppress all Python warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, _), (_, _) = mnist.load_data()\n",
        "\n",
        "# Normalize the pixel values to the range [-1, 1]\n",
        "x_train = x_train.astype('float32') / 127.5 - 1.\n",
        "x_train = np.expand_dims(x_train, axis=-1)\n",
        "\n",
        "# Print the shape of the data\n",
        "print(x_train.shape)"
      ],
      "metadata": {
        "id": "WgQjEhyP5I1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building the generator model**"
      ],
      "metadata": {
        "id": "jWMZAMdk5RsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization, Reshape\n",
        "\n",
        "# Define the generator model\n",
        "def build_generator():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, input_dim=100))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(512))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(1024))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(28 * 28 * 1, activation='tanh'))\n",
        "    model.add(Reshape((28, 28, 1)))\n",
        "    return model\n",
        "\n",
        "# Build the generator\n",
        "generator = build_generator()\n",
        "generator.summary()"
      ],
      "metadata": {
        "id": "J_WIwzNh5S5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building the discriminator model**"
      ],
      "metadata": {
        "id": "WP9FbaBH5V74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LeakyReLU\n",
        "\n",
        "# Define the discriminator model\n",
        "def build_discriminator():\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(28, 28, 1)))\n",
        "    model.add(Dense(512))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(256))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# Build and compile the discriminator\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "discriminator.summary()"
      ],
      "metadata": {
        "id": "x62ZrE195YtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building the GAN Model**\n",
        "\n",
        "Combine the generator and discriminator to create the GAN model using the Keras."
      ],
      "metadata": {
        "id": "35mph7gB5f2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Create the GAN by stacking the generator and the discriminator\n",
        "def build_gan(generator, discriminator):\n",
        "    discriminator.trainable = False\n",
        "    gan_input = Input(shape=(100,))\n",
        "    generated_image = generator(gan_input)\n",
        "    gan_output = discriminator(generated_image)\n",
        "    gan = Model(gan_input, gan_output)\n",
        "    gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    return gan\n",
        "\n",
        "# Build the GAN\n",
        "gan = build_gan(generator, discriminator)\n",
        "gan.summary()"
      ],
      "metadata": {
        "id": "HS9hlZw85o08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training the GAN**"
      ],
      "metadata": {
        "id": "EnQvSzmn5tND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define and compile the discriminator model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LeakyReLU, Flatten\n",
        "\n",
        "def build_discriminator():\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(28, 28, 1)))\n",
        "    model.add(Dense(512))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(256))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# Build and recompile the discriminator\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "discriminator.summary()\n"
      ],
      "metadata": {
        "id": "yr88XQsT5wEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "\n",
        "batch_size = 64\n",
        "epochs = 50\n",
        "sample_interval = 10\n",
        "\n",
        "# Adversarial ground truths\n",
        "real = np.ones((batch_size, 1))\n",
        "fake = np.zeros((batch_size, 1))\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    # Train the discriminator\n",
        "    idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
        "    real_images = x_train[idx]\n",
        "    noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "    generated_images = generator.predict(noise)\n",
        "    d_loss_real = discriminator.train_on_batch(real_images, real)\n",
        "    d_loss_fake = discriminator.train_on_batch(generated_images, fake)\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "    # Train the generator\n",
        "    noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "    g_loss = gan.train_on_batch(noise, real)\n",
        "\n",
        "    # Print the progress\n",
        "    if epoch % sample_interval == 0:\n",
        "        print(f\"{epoch} [D loss: {d_loss[0]}] [D accuracy: {100 * d_loss[1]}%] [G loss: {g_loss}]\")\n"
      ],
      "metadata": {
        "id": "sIgNsBbv56Yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assessing the Quality of Generated Images**\n",
        "\n",
        "1. Qualitative Assessment: Visual Inspection"
      ],
      "metadata": {
        "id": "Lp0wRCrG5yl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def sample_images(generator, epoch, num_images=25):\n",
        "    noise = np.random.normal(0, 1, (num_images, 100))\n",
        "    generated_images = generator.predict(noise)\n",
        "    generated_images = 0.5 * generated_images + 0.5  # Rescale to [0, 1]\n",
        "    fig, axs = plt.subplots(5, 5, figsize=(10, 10))\n",
        "    count = 0\n",
        "\n",
        "    for i in range(5):\n",
        "        for j in range(5):\n",
        "            axs[i, j].imshow(generated_images[count, :, :, 0], cmap='gray')\n",
        "            axs[i, j].axis('off')\n",
        "            count += 1\n",
        "    plt.show()\n",
        "\n",
        "# Sample images at the end of training\n",
        "sample_images(generator, epochs)"
      ],
      "metadata": {
        "id": "0pt7Zimw6G2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Quantitative Assessment: Metrics (Discriminator Accuracy)"
      ],
      "metadata": {
        "id": "rFXEWyuZ6XE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and print the discriminator accuracy on real vs. fake images\n",
        "noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "generated_images = generator.predict(noise)\n",
        "\n",
        "# Evaluate the discriminator on real images\n",
        "real_images = x_train[np.random.randint(0, x_train.shape[0], batch_size)]\n",
        "d_loss_real = discriminator.evaluate(real_images, np.ones((batch_size, 1)), verbose=0)\n",
        "\n",
        "# Evaluate the discriminator on fake images\n",
        "d_loss_fake = discriminator.evaluate(generated_images, np.zeros((batch_size, 1)), verbose=0)\n",
        "\n",
        "print(f\"Discriminator Accuracy on Real Images: {d_loss_real[1] * 100:.2f}%\")\n",
        "print(f\"Discriminator Accuracy on Fake Images: {d_loss_fake[1] * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "WsVElKYt6KOu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}